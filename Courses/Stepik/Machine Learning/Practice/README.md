# ML course by 7bits|it-lift

В этом репозитории коллекция материалов курсов в 7bits|it-lift:

1. Основы машинного обучения - это архив заданий [Coding Garden по машинному обучению](https://coding-garden.itlft.ru/).
2. ML на sklearn - архив [курса по ML](https://courses.itlft.ru/data-science) за 2018-2020 года.
3. Основы глубокого обучения - задания курса по реализации нейросетей с нуля на numpy.
4. Архитектуры нейросетей - задания курса по архитектурам сетей на Pytorch.

## Введение в ML

В заданиях вы будете реализовывать алгоритмы вручную на numpy. Необходимая теория будет распиана в jupyter ноутбуках.

Если вы студент из Омска, то можете получить консультации по курсу. Для этого зарегистрируйтесь на [Coding Garden для ML](https://coding-garden.itlft.ru/).

1. kNN
2. Линейная регрессия
3. kMeans
4. Наивный Байес и тексты
5. Деревья решений
6. Ансамбли моделей: Градиентный бустинг, Беггинг

## ML на sklearn

В заданиях используется sklearn, pandas, numpy, matplotlib, seaborn, keras.

Теория записана в виде [курса на stepik](https://stepik.org/course/8057/promo).

1. Таблицы с данными. Признаки и их типы. Законы распределения признаков и зависимость между признаками (тут же: коэффициент корреляции).
2. Проблема пропущенных данных. Методы восстановления. Восстановление с помощью метрик и коэфф корреляции. Приложение всего этого в рекомендательных системах.
3. Выбросы. Алгоритмы обнаружения выбросов. Критерии Пирса и Шавене (Chauvenet). Прочие методы обнаружения выбросов и обнаружение новизны в новой выборке (по сравнению со старой).
4. Кластеризация. Алгоритмы (метрические и k-means). Обнаружение выбросов с помощью кластеризации. 
5. Задача предсказания значения признака. Тренировочная и тестовая выборка. Общий план работы работы алгоритма предсказания (разбиение на тестовую и тренировочную выборку). Кросс-валидация. Переобучение.
6. Задача регрессии. Оценка качества МАЕ, МАРЕ и пр. Методы регрессии: линейная модель, метрические (типа ближайшего соседа).
7. Задача классификации. Общая теория задач бинарной классификации. Таблица ошибок. Ошибки первого и второго рода. Показатели качества. Самый простой классификатор.
8. Методы классификации (изучаем по порядку): метрические, Байес, деревья, случайный лес, SVM. Для каждого алгоритма разбирается пример его работы.
9. Предсказание вероятности принадлежности классам. Когда такой подход оправдан. Показатели качества для таких алгоритмов (ROC, AUC)
10. Логистическая регрессия (она как раз и предсказывает вероятность принадлежности классам).
11. Анализ множества признаков. Информативность признака. Синтез новых признаков. Методы преобразования признаков. Метод главных компонент и другие методы, связанные с разложение матрицы признаков.
12. Изменение числа объектов (при несбалансированной выборке). Синтетические образы, расщепление образа на несколько других, oversampling and undersampling.

## Основы глубокого обучения

В заданиях вы будете писать свою библиотеку автодифференцирования и базовый набор слоев для нейросети.

Теория по курсу [Машинного обучения в ОмГУ на ИМИТ](https://vk.com/club115689968). (скоро будет перезаписана в хорошем качестве)

_Появится в сентябре 2021_

## Архитектуры нейросетей

В заданиях вы будете писать архитектуры нейросетей на pytorch и тренировать сетки решать задачи.

_Появится в декабре 2021_
